## PyTorch 공부

Owhen 님 추천 : [딥러닝 추천 자료](https://github.com/huggingface/transformers)

- Fine-tuning a pretrained model
- [Transformers 공부](https://github.com/huggingface/transformers/tree/master/examples/pytorch) --> Colab 실행하면서 **PyTorch** 공식 문서도 함께 공부하기
  - `Token Classification`
- Transformer-addons
  - PPT 자료 참고
  - [42 Maru Github Repository](https://github.com/42maru-ai/42maru-transformers-addons) --> 코드 참고
- Training a classifier



Jerry 님 추천 : [PyTorch Tutorial](https://pytorch.org/tutorials/)

- PyTorch Recipes



참고 :

- `Quantization`
  - Quantization for deep learning is the process of approximating a neural network that uses floating-point numbers by a neural network of low bit width numbers. 
  - (양자화) -> 프로젝트 진행 시간을 단축시키기 위해 사용 **but** 내가 진행하는 프로젝트 수준에서는 굳이 필요하지 않음(정확도 하락함)
- `42 Maru/Kamino` Repo 참고(Hanmi Classifier 에서도 쓰이는 내용)



## 페어 코딩

> Ohwen 님과 PyTorch 연습 프로젝트 이어감

```python
# GPU 상태 확인 명령어
nvidia-smi

# 실시간(예 : 0.5초마다) 상태 확인 명령어
watch -d -n 0.5 nvidia-smi
```



- Training Accuracy 가 계속 똑같은 문제 --> `overfitting` 이 원인!! 
